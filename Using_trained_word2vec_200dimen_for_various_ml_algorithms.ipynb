{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhlLszrhzZ-9",
        "outputId": "d02ff85c-67f9-4a7c-a799-ebe24edf137b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m749.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iRQUZim0cHOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim==4.3.3 numpy==1.24.3 scipy==1.10.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "DGvJ1mBr1pa_",
        "outputId": "47cf5b67-b837-4da6-d221-5fcf2eb66c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim==4.3.3\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting scipy==1.10.1\n",
            "  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.3) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.3) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.24.3 scipy-1.10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "61c637bf85f84d55b0685f87733a4a6f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Word2Vec Model"
      ],
      "metadata": {
        "id": "ZKFkA3U2fIdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec.load('/content/youtube_word2vec_200dim.model')\n",
        "print(\"Word2Vec model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0emx0yGzlBq",
        "outputId": "4bdebff3-59ed-4be7-a47a-3297f696e5a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Tokenized excel file"
      ],
      "metadata": {
        "id": "kDOCwAhEfOEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "# Load dataset\n",
        "df = pd.read_excel(\"/content/sentiment_tokens.xlsx\")\n",
        "\n",
        "# Safe parsing: Convert stringified list to actual list\n",
        "def safe_parse(x):\n",
        "    try:\n",
        "        return ast.literal_eval(x)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing row: {x[:80]}... -> {e}\")\n",
        "        return []\n",
        "\n",
        "df['tokens'] = df['tokens'].apply(safe_parse)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coo9vtxf1y1u",
        "outputId": "03d1d8a7-f4a3-445a-ef9c-ffdae2fd920c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error parsing row: ['sidney', 'powel', 'credible', 'rolling_on_the_floor_laughing', 'rolling_on_the... -> '[' was never closed (<unknown>, line 1)\n",
            "Error parsing row: ['relateable', 'party_popper', 'party_popper', 'party_popper', 'party_popper', '... -> '[' was never closed (<unknown>, line 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52kQirAr3N1b",
        "outputId": "68b054c3-9ad5-4e8a-e1f7-f8e7a3a7dd38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(963055, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['tokens'].iloc[0])\n",
        "print(type(df['tokens'].iloc[0]))  # Should now be <class 'list'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlOqOxIT20Qn",
        "outputId": "f9a624cf-fcc1-4873-c5b2-c5fa8ced7417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['anyone', 'know', 'movie', '?']\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Sentence Embeddings (Mean Pooling)"
      ],
      "metadata": {
        "id": "_lcv8AJBfZtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOCK 3: Create Sentence Embeddings (Mean Pooling)\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def sentence_embedding(tokens, model):\n",
        "    vectors = [model[word] for word in tokens if word in model]\n",
        "    if not vectors:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# Build feature matrix X\n",
        "X = np.array([sentence_embedding(tokens, model.wv) for tokens in tqdm(df['tokens'])])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "agh_j6fX14W3",
        "outputId": "ff6932e9-9dcd-493f-a026-de4da8a0be75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 963055/963055 [00:46<00:00, 20884.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2I-DqWf4VAL",
        "outputId": "209b8fb9-d098-413f-af07-e5981d87bae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.140857  , -0.03284824, -0.06331647, ..., -0.17949659,\n",
              "        -0.12216   , -0.14116634],\n",
              "       [ 0.04501991,  0.01944968,  0.2220042 , ..., -0.11654779,\n",
              "         0.0016356 ,  0.05193744],\n",
              "       [ 0.07193813,  0.07645284, -0.25776291, ..., -0.05467423,\n",
              "         0.0794609 , -0.17267951],\n",
              "       ...,\n",
              "       [-0.0154625 ,  0.11621616,  0.0079456 , ..., -0.12408878,\n",
              "         0.07909657,  0.13012357],\n",
              "       [ 0.02637978,  0.01195572,  0.09002316, ..., -0.19372623,\n",
              "         0.01496309,  0.01822849],\n",
              "       [-0.17009021,  0.01112945, -0.27889833, ..., -0.32648149,\n",
              "         0.15604128,  0.17522995]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvLnn3H55xXB",
        "outputId": "3f57d599-8eb4-4878-a5a8-a8d1b47bb81c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Sentiment', 'tokens'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['Sentiment'].values"
      ],
      "metadata": {
        "id": "9QIAUUxN4YNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLzN0hPx6QdN",
        "outputId": "711bbf7f-9350-4f30-c34e-b98d6470747b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Neutral', 'Positive', 'Neutral', ..., 'Negative', 'Neutral',\n",
              "       'Neutral'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOCK 4: Encode Labels & Train-Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"Classes: {label_encoder.classes_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BRrCP1w1-BI",
        "outputId": "2ab5e8cf-fb38-4311-c004-62cda5a1b1bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['Negative' 'Neutral' 'Positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsatHzso6T_E",
        "outputId": "8dd772d3-ab77-492b-a9e6-4b5d121aaa2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 1, ..., 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Traning the Sentence Embedding created on ML models"
      ],
      "metadata": {
        "id": "Beu3qd5qf64z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deafult Logistic Regression Accuracy"
      ],
      "metadata": {
        "id": "Iky8goAVfd_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  BLOCK 5: Train Models & Evaluate\n"
      ],
      "metadata": {
        "id": "dvHbQBUa2EoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred_log = log_reg.predict(X_test)\n",
        "\n",
        "print(\"=== Logistic Regression ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_log))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_log, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW0-tsZT2I3Z",
        "outputId": "267d0fac-8589-4141-be07-beee8adbea6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Logistic Regression ===\n",
            "Accuracy: 0.6603361178748878\n",
            "Confusion Matrix:\n",
            " [[45173 14055  5960]\n",
            " [15940 36136 10055]\n",
            " [ 9113 10300 45879]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.64      0.69      0.67     65188\n",
            "     Neutral       0.60      0.58      0.59     62131\n",
            "    Positive       0.74      0.70      0.72     65292\n",
            "\n",
            "    accuracy                           0.66    192611\n",
            "   macro avg       0.66      0.66      0.66    192611\n",
            "weighted avg       0.66      0.66      0.66    192611\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Default Random Forest Accuracy"
      ],
      "metadata": {
        "id": "-rCT1eh_fk1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "print(\"=== Random Forest ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "id": "XPsooOWx2Kj5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "e9c38973-22bb-43e6-d415-2c381bc8584f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-95aec5e0b532>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_pred_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1983\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1985\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1911\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         tree._fit(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Default Xgboost Accuracy"
      ],
      "metadata": {
        "id": "516oRpaOfxrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoostXG\n",
        "import xgboost as xgb\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    objective='multi:softmax', num_class=len(label_encoder.classes_),\n",
        "    eval_metric='mlogloss', use_label_encoder=False\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "print(\"=== XGBoost ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_xgb, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "id": "4oVATJJW2NUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf5ffc6-31c6-4bc6-c74e-76ea41977b52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:06:53] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== XGBoost ===\n",
            "Accuracy: 0.6852256620857583\n",
            "Confusion Matrix:\n",
            " [[47566 12287  5335]\n",
            " [15458 37879  8794]\n",
            " [ 9348  9407 46537]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.66      0.73      0.69     65188\n",
            "     Neutral       0.64      0.61      0.62     62131\n",
            "    Positive       0.77      0.71      0.74     65292\n",
            "\n",
            "    accuracy                           0.69    192611\n",
            "   macro avg       0.69      0.68      0.68    192611\n",
            "weighted avg       0.69      0.69      0.69    192611\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Default LightGBM Accuracy"
      ],
      "metadata": {
        "id": "kVN4nznnf12i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LightGBM\n",
        "import lightgbm as lgb\n",
        "\n",
        "lgb_model = lgb.LGBMClassifier(objective='multiclass', num_class=len(label_encoder.classes_))\n",
        "lgb_model.fit(X_train, y_train)\n",
        "y_pred_lgb = lgb_model.predict(X_test)\n",
        "\n",
        "print(\"=== LightGBM ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lgb))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lgb))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lgb, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "id": "PqgmK9fE2R7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7d42c9-1099-4975-f3e0-b5fed56103a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.236304 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 51000\n",
            "[LightGBM] [Info] Number of data points in the train set: 770444, number of used features: 200\n",
            "[LightGBM] [Info] Start training from score -1.083397\n",
            "[LightGBM] [Info] Start training from score -1.131428\n",
            "[LightGBM] [Info] Start training from score -1.081803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LightGBM ===\n",
            "Accuracy: 0.6715296634148621\n",
            "Confusion Matrix:\n",
            " [[46826 13101  5261]\n",
            " [16248 37293  8590]\n",
            " [10045 10022 45225]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.64      0.72      0.68     65188\n",
            "     Neutral       0.62      0.60      0.61     62131\n",
            "    Positive       0.77      0.69      0.73     65292\n",
            "\n",
            "    accuracy                           0.67    192611\n",
            "   macro avg       0.67      0.67      0.67    192611\n",
            "weighted avg       0.68      0.67      0.67    192611\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === XGBoost (GPU) ===\n",
        "print(\"\\n=== XGBoost (GPU) ===\")\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    tree_method='gpu_hist',\n",
        "    predictor='gpu_predictor',\n",
        "    max_depth=10,\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    verbosity=1,\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_xgb, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1dsgzt_-h_v",
        "outputId": "d9505fd9-8c9e-4fa2-ea39-3c65a95ca729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== XGBoost (GPU) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:22:12] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:22:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:22:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:22:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6961025071257613\n",
            "Confusion Matrix:\n",
            " [[48031 12098  5059]\n",
            " [14569 39126  8436]\n",
            " [ 8786  9586 46920]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.67      0.74      0.70     65188\n",
            "     Neutral       0.64      0.63      0.64     62131\n",
            "    Positive       0.78      0.72      0.75     65292\n",
            "\n",
            "    accuracy                           0.70    192611\n",
            "   macro avg       0.70      0.70      0.70    192611\n",
            "weighted avg       0.70      0.70      0.70    192611\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === LightGBM (GPU optional) ===\n",
        "print(\"\\n=== LightGBM ===\")\n",
        "lgbm_model = lgb.LGBMClassifier(\n",
        "    device='gpu',     # Remove this line if using CPU\n",
        "    max_depth=10,\n",
        "    num_leaves=31,\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "y_pred_lgb = lgbm_model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lgb))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lgb))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lgb, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd1YsV7Z_Bd9",
        "outputId": "54687c24-a671-481f-ed9e-5ea06fe38028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== LightGBM ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 51000\n",
            "[LightGBM] [Info] Number of data points in the train set: 770444, number of used features: 200\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 200 dense feature groups (146.95 MB) transferred to GPU in 0.207184 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score -1.083397\n",
            "[LightGBM] [Info] Start training from score -1.131428\n",
            "[LightGBM] [Info] Start training from score -1.081803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6713323745788143\n",
            "Confusion Matrix:\n",
            " [[46750 13218  5220]\n",
            " [16224 37301  8606]\n",
            " [ 9993 10044 45255]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.64      0.72      0.68     65188\n",
            "     Neutral       0.62      0.60      0.61     62131\n",
            "    Positive       0.77      0.69      0.73     65292\n",
            "\n",
            "    accuracy                           0.67    192611\n",
            "   macro avg       0.67      0.67      0.67    192611\n",
            "weighted avg       0.68      0.67      0.67    192611\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Logistic Regression ===\n",
        "print(\"\\n=== Logistic Regression ===\")\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_lr = logreg.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul3Xc1jJ_HNU",
        "outputId": "3d22fde2-734f-4040-c2f9-728a08584c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression ===\n",
            "Accuracy: 0.6603361178748878\n",
            "Confusion Matrix:\n",
            " [[45173 14055  5960]\n",
            " [15940 36136 10055]\n",
            " [ 9113 10300 45879]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.64      0.69      0.67     65188\n",
            "     Neutral       0.60      0.58      0.59     62131\n",
            "    Positive       0.74      0.70      0.72     65292\n",
            "\n",
            "    accuracy                           0.66    192611\n",
            "   macro avg       0.66      0.66      0.66    192611\n",
            "weighted avg       0.66      0.66      0.66    192611\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LightGBM and Xgboost for Deeper Hyper-parameter Tuning"
      ],
      "metadata": {
        "id": "W15ZinEOgQ5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper-parameter tuning via Optuna"
      ],
      "metadata": {
        "id": "7khTEnXMC1VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna lightgbm xgboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7GcnayeC3sa",
        "outputId": "c221d039-9569-48b2-85f5-8f8753c8fe0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.10.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.1 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n"
      ],
      "metadata": {
        "id": "yFUkWG2IC6xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, stratify=y, random_state=42)\n"
      ],
      "metadata": {
        "id": "u2baPAnnDARE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7u5SRRuDEwC",
        "outputId": "16471576-631a-495f-8f81-3268610af775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((770444, 200), (192611, 200), (770444,), (192611,))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtGccbTADhZK",
        "outputId": "57d56c5f-db76-48cc-9b35-d0bc92a0f314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 1, ..., 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LightGBM hyper-parameter tuning"
      ],
      "metadata": {
        "id": "AcbS2SVhC7qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from lightgbm import early_stopping, log_evaluation\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "vzq6keYyv3Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Make sure y, X_train, X_val, y_train, y_val are defined\n",
        "\n",
        "# -------------------------------\n",
        "# LIGHTGBM OPTUNA TUNING + SAVE\n",
        "# -------------------------------\n",
        "def objective_lgb(trial):\n",
        "    param = {\n",
        "        'objective': 'multiclass',\n",
        "        'metric': 'multi_logloss',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_class': len(set(y)),\n",
        "        'verbosity': -1,\n",
        "        'device': 'gpu',\n",
        "        'gpu_platform_id': 0,\n",
        "        'gpu_device_id': 0,\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 31, 256),\n",
        "        'max_depth': trial.suggest_int('max_depth', 4, 16),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True)\n",
        "    }\n",
        "\n",
        "    train_set = lgb.Dataset(X_train, label=y_train)\n",
        "    val_set = lgb.Dataset(X_val, label=y_val)\n",
        "\n",
        "    model = lgb.train(\n",
        "        param,\n",
        "        train_set,\n",
        "        valid_sets=[val_set],\n",
        "        callbacks=[\n",
        "            early_stopping(stopping_rounds=50),\n",
        "            log_evaluation(period=10)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    preds = model.predict(X_val)\n",
        "    pred_labels = preds.argmax(axis=1)\n",
        "    return accuracy_score(y_val, pred_labels)\n",
        "\n",
        "# Run LightGBM tuning\n",
        "study_lgb = optuna.create_study(direction='maximize')\n",
        "study_lgb.optimize(objective_lgb, n_trials=50)\n",
        "print(\"✅ Best LightGBM params:\", study_lgb.best_params)\n",
        "\n",
        "# Train final LightGBM model with best params\n",
        "best_params_lgb = study_lgb.best_params\n",
        "best_params_lgb.update({\n",
        "    'objective': 'multiclass',\n",
        "    'metric': 'multi_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_class': len(set(y)),\n",
        "    'verbosity': -1,\n",
        "    'device': 'gpu',\n",
        "    'gpu_platform_id': 0,\n",
        "    'gpu_device_id': 0\n",
        "})\n",
        "\n",
        "train_set = lgb.Dataset(X_train, label=y_train)\n",
        "val_set = lgb.Dataset(X_val, label=y_val)\n",
        "\n",
        "final_lgb = lgb.train(\n",
        "    best_params_lgb,\n",
        "    train_set,\n",
        "    valid_sets=[val_set],\n",
        "    callbacks=[\n",
        "        early_stopping(stopping_rounds=50),\n",
        "        log_evaluation(period=10)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Save final LightGBM model\n",
        "joblib.dump(final_lgb, 'best_lightgbm.pkl')\n",
        "print(\"✅ LightGBM model saved to best_lightgbm.pkl\")"
      ],
      "metadata": {
        "id": "Y-zTTBGEDMw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98136a43-5aa9-46f2-8eb3-89aebb4e4769"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 14:52:32,134] A new study created in memory with name: no-name-40e4a345-e895-4afb-8d6c-b5952470ffba\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.817268\n",
            "[20]\tvalid_0's multi_logloss: 0.775915\n",
            "[30]\tvalid_0's multi_logloss: 0.757608\n",
            "[40]\tvalid_0's multi_logloss: 0.746918\n",
            "[50]\tvalid_0's multi_logloss: 0.739437\n",
            "[60]\tvalid_0's multi_logloss: 0.733955\n",
            "[70]\tvalid_0's multi_logloss: 0.729262\n",
            "[80]\tvalid_0's multi_logloss: 0.725492\n",
            "[90]\tvalid_0's multi_logloss: 0.722441\n",
            "[100]\tvalid_0's multi_logloss: 0.720059\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.720059\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 14:53:45,995] Trial 0 finished with value: 0.6816588876024734 and parameters: {'learning_rate': 0.2974076182133885, 'num_leaves': 49, 'max_depth': 5, 'min_child_samples': 86, 'subsample': 0.7605254586821979, 'colsample_bytree': 0.5575807832927954, 'reg_alpha': 0.012893076468883968, 'reg_lambda': 0.17026608359845577}. Best is trial 0 with value: 0.6816588876024734.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.974519\n",
            "[20]\tvalid_0's multi_logloss: 0.904064\n",
            "[30]\tvalid_0's multi_logloss: 0.859508\n",
            "[40]\tvalid_0's multi_logloss: 0.829644\n",
            "[50]\tvalid_0's multi_logloss: 0.808655\n",
            "[60]\tvalid_0's multi_logloss: 0.793129\n",
            "[70]\tvalid_0's multi_logloss: 0.781096\n",
            "[80]\tvalid_0's multi_logloss: 0.771485\n",
            "[90]\tvalid_0's multi_logloss: 0.763596\n",
            "[100]\tvalid_0's multi_logloss: 0.756956\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.756956\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 14:55:33,544] Trial 1 finished with value: 0.66771368198078 and parameters: {'learning_rate': 0.039771130450381234, 'num_leaves': 177, 'max_depth': 8, 'min_child_samples': 66, 'subsample': 0.8909021176741665, 'colsample_bytree': 0.6671628852333613, 'reg_alpha': 1.8484405828483973e-08, 'reg_lambda': 1.5969120042800658e-07}. Best is trial 0 with value: 0.6816588876024734.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 1.03953\n",
            "[20]\tvalid_0's multi_logloss: 0.995125\n",
            "[30]\tvalid_0's multi_logloss: 0.960412\n",
            "[40]\tvalid_0's multi_logloss: 0.932657\n",
            "[50]\tvalid_0's multi_logloss: 0.909992\n",
            "[60]\tvalid_0's multi_logloss: 0.891333\n",
            "[70]\tvalid_0's multi_logloss: 0.875787\n",
            "[80]\tvalid_0's multi_logloss: 0.862614\n",
            "[90]\tvalid_0's multi_logloss: 0.851188\n",
            "[100]\tvalid_0's multi_logloss: 0.841344\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.841344\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 14:56:45,683] Trial 2 finished with value: 0.6386447295325812 and parameters: {'learning_rate': 0.01793833558648358, 'num_leaves': 253, 'max_depth': 6, 'min_child_samples': 87, 'subsample': 0.9552928206157524, 'colsample_bytree': 0.8763928694746412, 'reg_alpha': 5.026932232227248e-06, 'reg_lambda': 8.760561802633928}. Best is trial 0 with value: 0.6816588876024734.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 1.08933\n",
            "[20]\tvalid_0's multi_logloss: 1.08074\n",
            "[30]\tvalid_0's multi_logloss: 1.07246\n",
            "[40]\tvalid_0's multi_logloss: 1.06444\n",
            "[50]\tvalid_0's multi_logloss: 1.05671\n",
            "[60]\tvalid_0's multi_logloss: 1.0492\n",
            "[70]\tvalid_0's multi_logloss: 1.04191\n",
            "[80]\tvalid_0's multi_logloss: 1.0349\n",
            "[90]\tvalid_0's multi_logloss: 1.02808\n",
            "[100]\tvalid_0's multi_logloss: 1.02154\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 1.02154\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 14:58:41,169] Trial 3 finished with value: 0.6254834874436039 and parameters: {'learning_rate': 0.0022270107527222706, 'num_leaves': 128, 'max_depth': 13, 'min_child_samples': 61, 'subsample': 0.8577474819916142, 'colsample_bytree': 0.7418048429012501, 'reg_alpha': 0.00024357869003356305, 'reg_lambda': 0.002095914650153677}. Best is trial 0 with value: 0.6816588876024734.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.820515\n",
            "[20]\tvalid_0's multi_logloss: 0.774132\n",
            "[30]\tvalid_0's multi_logloss: 0.75344\n",
            "[40]\tvalid_0's multi_logloss: 0.741563\n",
            "[50]\tvalid_0's multi_logloss: 0.733442\n",
            "[60]\tvalid_0's multi_logloss: 0.727089\n",
            "[70]\tvalid_0's multi_logloss: 0.722426\n",
            "[80]\tvalid_0's multi_logloss: 0.718641\n",
            "[90]\tvalid_0's multi_logloss: 0.715452\n",
            "[100]\tvalid_0's multi_logloss: 0.712797\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.712797\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 14:59:49,505] Trial 4 finished with value: 0.6852568129546079 and parameters: {'learning_rate': 0.22519156543097418, 'num_leaves': 218, 'max_depth': 6, 'min_child_samples': 14, 'subsample': 0.5524018297578352, 'colsample_bytree': 0.8481135897435945, 'reg_alpha': 0.0007848821361461149, 'reg_lambda': 0.009221619998084542}. Best is trial 4 with value: 0.6852568129546079.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 1.03996\n",
            "[20]\tvalid_0's multi_logloss: 0.995537\n",
            "[30]\tvalid_0's multi_logloss: 0.96022\n",
            "[40]\tvalid_0's multi_logloss: 0.931742\n",
            "[50]\tvalid_0's multi_logloss: 0.908536\n",
            "[60]\tvalid_0's multi_logloss: 0.889219\n",
            "[70]\tvalid_0's multi_logloss: 0.873074\n",
            "[80]\tvalid_0's multi_logloss: 0.859444\n",
            "[90]\tvalid_0's multi_logloss: 0.847741\n",
            "[100]\tvalid_0's multi_logloss: 0.83762\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.83762\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:01:14,510] Trial 5 finished with value: 0.6423828337945392 and parameters: {'learning_rate': 0.01755154804315452, 'num_leaves': 61, 'max_depth': 14, 'min_child_samples': 66, 'subsample': 0.9171881925151886, 'colsample_bytree': 0.7303290866680088, 'reg_alpha': 0.00991100395332814, 'reg_lambda': 0.0002595804215802485}. Best is trial 4 with value: 0.6852568129546079.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 1.08765\n",
            "[20]\tvalid_0's multi_logloss: 1.07752\n",
            "[30]\tvalid_0's multi_logloss: 1.06778\n",
            "[40]\tvalid_0's multi_logloss: 1.05841\n",
            "[50]\tvalid_0's multi_logloss: 1.04943\n",
            "[60]\tvalid_0's multi_logloss: 1.04075\n",
            "[70]\tvalid_0's multi_logloss: 1.03238\n",
            "[80]\tvalid_0's multi_logloss: 1.02434\n",
            "[90]\tvalid_0's multi_logloss: 1.01657\n",
            "[100]\tvalid_0's multi_logloss: 1.00914\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 1.00914\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:03:47,148] Trial 6 finished with value: 0.6342887997051051 and parameters: {'learning_rate': 0.002507554890391266, 'num_leaves': 231, 'max_depth': 10, 'min_child_samples': 72, 'subsample': 0.5558120376316456, 'colsample_bytree': 0.7500361806305476, 'reg_alpha': 0.00040234980606156784, 'reg_lambda': 0.14314256298646238}. Best is trial 4 with value: 0.6852568129546079.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 1.03865\n",
            "[20]\tvalid_0's multi_logloss: 0.992199\n",
            "[30]\tvalid_0's multi_logloss: 0.95533\n",
            "[40]\tvalid_0's multi_logloss: 0.925422\n",
            "[50]\tvalid_0's multi_logloss: 0.900922\n",
            "[60]\tvalid_0's multi_logloss: 0.880554\n",
            "[70]\tvalid_0's multi_logloss: 0.863523\n",
            "[80]\tvalid_0's multi_logloss: 0.849221\n",
            "[90]\tvalid_0's multi_logloss: 0.836885\n",
            "[100]\tvalid_0's multi_logloss: 0.826345\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.826345\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:05:30,193] Trial 7 finished with value: 0.6520759458182555 and parameters: {'learning_rate': 0.017147240997873476, 'num_leaves': 133, 'max_depth': 9, 'min_child_samples': 48, 'subsample': 0.912968146591841, 'colsample_bytree': 0.5190607138261798, 'reg_alpha': 0.0002762078322943501, 'reg_lambda': 1.0913949419292122}. Best is trial 4 with value: 0.6852568129546079.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.944238\n",
            "[20]\tvalid_0's multi_logloss: 0.879946\n",
            "[30]\tvalid_0's multi_logloss: 0.845334\n",
            "[40]\tvalid_0's multi_logloss: 0.823271\n",
            "[50]\tvalid_0's multi_logloss: 0.808132\n",
            "[60]\tvalid_0's multi_logloss: 0.796943\n",
            "[70]\tvalid_0's multi_logloss: 0.788023\n",
            "[80]\tvalid_0's multi_logloss: 0.780918\n",
            "[90]\tvalid_0's multi_logloss: 0.774884\n",
            "[100]\tvalid_0's multi_logloss: 0.76988\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.76988\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:06:20,453] Trial 8 finished with value: 0.6586799300143813 and parameters: {'learning_rate': 0.08302557510458468, 'num_leaves': 236, 'max_depth': 4, 'min_child_samples': 53, 'subsample': 0.9612771248569068, 'colsample_bytree': 0.7037616623578666, 'reg_alpha': 1.4070287348982518e-08, 'reg_lambda': 3.3498310613951356}. Best is trial 4 with value: 0.6852568129546079.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 1.03623\n",
            "[20]\tvalid_0's multi_logloss: 0.988857\n",
            "[30]\tvalid_0's multi_logloss: 0.95143\n",
            "[40]\tvalid_0's multi_logloss: 0.92126\n",
            "[50]\tvalid_0's multi_logloss: 0.896667\n",
            "[60]\tvalid_0's multi_logloss: 0.876148\n",
            "[70]\tvalid_0's multi_logloss: 0.858993\n",
            "[80]\tvalid_0's multi_logloss: 0.844416\n",
            "[90]\tvalid_0's multi_logloss: 0.831983\n",
            "[100]\tvalid_0's multi_logloss: 0.821261\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.821261\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:09:26,287] Trial 9 finished with value: 0.6520915212526802 and parameters: {'learning_rate': 0.015861327962547703, 'num_leaves': 234, 'max_depth': 16, 'min_child_samples': 41, 'subsample': 0.8466917503464884, 'colsample_bytree': 0.9802906474998063, 'reg_alpha': 3.2225941179084286e-07, 'reg_lambda': 0.31762541547494777}. Best is trial 4 with value: 0.6852568129546079.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.846482\n",
            "[20]\tvalid_0's multi_logloss: 0.785741\n",
            "[30]\tvalid_0's multi_logloss: 0.759227\n",
            "[40]\tvalid_0's multi_logloss: 0.743835\n",
            "[50]\tvalid_0's multi_logloss: 0.733505\n",
            "[60]\tvalid_0's multi_logloss: 0.726082\n",
            "[70]\tvalid_0's multi_logloss: 0.720155\n",
            "[80]\tvalid_0's multi_logloss: 0.715344\n",
            "[90]\tvalid_0's multi_logloss: 0.711718\n",
            "[100]\tvalid_0's multi_logloss: 0.708501\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.708501\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:11:17,355] Trial 10 finished with value: 0.6882888308559739 and parameters: {'learning_rate': 0.13318215507905476, 'num_leaves': 182, 'max_depth': 8, 'min_child_samples': 13, 'subsample': 0.5192016835706874, 'colsample_bytree': 0.8893136963018567, 'reg_alpha': 7.731322898254129, 'reg_lambda': 8.97606796243808e-06}. Best is trial 10 with value: 0.6882888308559739.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.794922\n",
            "[20]\tvalid_0's multi_logloss: 0.754654\n",
            "[30]\tvalid_0's multi_logloss: 0.737653\n",
            "[40]\tvalid_0's multi_logloss: 0.727508\n",
            "[50]\tvalid_0's multi_logloss: 0.720655\n",
            "[60]\tvalid_0's multi_logloss: 0.715918\n",
            "[70]\tvalid_0's multi_logloss: 0.711817\n",
            "[80]\tvalid_0's multi_logloss: 0.708528\n",
            "[90]\tvalid_0's multi_logloss: 0.705983\n",
            "[100]\tvalid_0's multi_logloss: 0.703721\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.703721\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:12:42,490] Trial 11 finished with value: 0.6899554023394302 and parameters: {'learning_rate': 0.2736763136325603, 'num_leaves': 187, 'max_depth': 7, 'min_child_samples': 10, 'subsample': 0.5045542044527518, 'colsample_bytree': 0.8965600150655003, 'reg_alpha': 9.538652695275518, 'reg_lambda': 9.979406380125947e-06}. Best is trial 11 with value: 0.6899554023394302.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.866888\n",
            "[20]\tvalid_0's multi_logloss: 0.801229\n",
            "[30]\tvalid_0's multi_logloss: 0.77102\n",
            "[40]\tvalid_0's multi_logloss: 0.753493\n",
            "[50]\tvalid_0's multi_logloss: 0.741612\n",
            "[60]\tvalid_0's multi_logloss: 0.733139\n",
            "[70]\tvalid_0's multi_logloss: 0.726545\n",
            "[80]\tvalid_0's multi_logloss: 0.721448\n",
            "[90]\tvalid_0's multi_logloss: 0.717163\n",
            "[100]\tvalid_0's multi_logloss: 0.713612\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.713612\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:14:37,996] Trial 12 finished with value: 0.6862484489463219 and parameters: {'learning_rate': 0.10997475084568008, 'num_leaves': 174, 'max_depth': 8, 'min_child_samples': 10, 'subsample': 0.6333214248959615, 'colsample_bytree': 0.9907841937709034, 'reg_alpha': 6.449228285413934, 'reg_lambda': 6.3669311173826735e-06}. Best is trial 11 with value: 0.6899554023394302.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.855448\n",
            "[20]\tvalid_0's multi_logloss: 0.790949\n",
            "[30]\tvalid_0's multi_logloss: 0.762167\n",
            "[40]\tvalid_0's multi_logloss: 0.745264\n",
            "[50]\tvalid_0's multi_logloss: 0.734152\n",
            "[60]\tvalid_0's multi_logloss: 0.725876\n",
            "[70]\tvalid_0's multi_logloss: 0.719355\n",
            "[80]\tvalid_0's multi_logloss: 0.714134\n",
            "[90]\tvalid_0's multi_logloss: 0.709829\n",
            "[100]\tvalid_0's multi_logloss: 0.706224\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.706224\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:16:58,517] Trial 13 finished with value: 0.6895452492329098 and parameters: {'learning_rate': 0.11882288413353986, 'num_leaves': 181, 'max_depth': 12, 'min_child_samples': 28, 'subsample': 0.6766061225295161, 'colsample_bytree': 0.8668902129806352, 'reg_alpha': 9.573501732709817, 'reg_lambda': 1.0615342171692177e-05}. Best is trial 11 with value: 0.6899554023394302.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.902351\n",
            "[20]\tvalid_0's multi_logloss: 0.82786\n",
            "[30]\tvalid_0's multi_logloss: 0.790665\n",
            "[40]\tvalid_0's multi_logloss: 0.768711\n",
            "[50]\tvalid_0's multi_logloss: 0.753989\n",
            "[60]\tvalid_0's multi_logloss: 0.74335\n",
            "[70]\tvalid_0's multi_logloss: 0.735173\n",
            "[80]\tvalid_0's multi_logloss: 0.728663\n",
            "[90]\tvalid_0's multi_logloss: 0.723332\n",
            "[100]\tvalid_0's multi_logloss: 0.718747\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.718747\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:19:32,121] Trial 14 finished with value: 0.6835538987908271 and parameters: {'learning_rate': 0.07616348778321982, 'num_leaves': 198, 'max_depth': 12, 'min_child_samples': 30, 'subsample': 0.6570704587254748, 'colsample_bytree': 0.8131253529747628, 'reg_alpha': 0.35850771067531034, 'reg_lambda': 1.0708501185342604e-08}. Best is trial 11 with value: 0.6899554023394302.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.9766\n",
            "[20]\tvalid_0's multi_logloss: 0.90779\n",
            "[30]\tvalid_0's multi_logloss: 0.864559\n",
            "[40]\tvalid_0's multi_logloss: 0.835347\n",
            "[50]\tvalid_0's multi_logloss: 0.8146\n",
            "[60]\tvalid_0's multi_logloss: 0.799106\n",
            "[70]\tvalid_0's multi_logloss: 0.787072\n",
            "[80]\tvalid_0's multi_logloss: 0.77741\n",
            "[90]\tvalid_0's multi_logloss: 0.769516\n",
            "[100]\tvalid_0's multi_logloss: 0.762761\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.762761\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:21:26,318] Trial 15 finished with value: 0.6641105648171703 and parameters: {'learning_rate': 0.04010737822006402, 'num_leaves': 101, 'max_depth': 12, 'min_child_samples': 29, 'subsample': 0.6931649802040128, 'colsample_bytree': 0.936980596020969, 'reg_alpha': 0.3034184024879293, 'reg_lambda': 1.8447772303959477e-05}. Best is trial 11 with value: 0.6899554023394302.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.817885\n",
            "[20]\tvalid_0's multi_logloss: 0.765337\n",
            "[30]\tvalid_0's multi_logloss: 0.742774\n",
            "[40]\tvalid_0's multi_logloss: 0.729741\n",
            "[50]\tvalid_0's multi_logloss: 0.720813\n",
            "[60]\tvalid_0's multi_logloss: 0.714185\n",
            "[70]\tvalid_0's multi_logloss: 0.70932\n",
            "[80]\tvalid_0's multi_logloss: 0.705327\n",
            "[90]\tvalid_0's multi_logloss: 0.702302\n",
            "[100]\tvalid_0's multi_logloss: 0.699634\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.699634\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:23:29,104] Trial 16 finished with value: 0.692738213289999 and parameters: {'learning_rate': 0.17696581518849255, 'num_leaves': 150, 'max_depth': 11, 'min_child_samples': 30, 'subsample': 0.7729845912315098, 'colsample_bytree': 0.8058082817698655, 'reg_alpha': 0.4397040593821415, 'reg_lambda': 5.034537900836039e-07}. Best is trial 16 with value: 0.692738213289999.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 1.08082\n",
            "[20]\tvalid_0's multi_logloss: 1.06477\n",
            "[30]\tvalid_0's multi_logloss: 1.04973\n",
            "[40]\tvalid_0's multi_logloss: 1.03571\n",
            "[50]\tvalid_0's multi_logloss: 1.02259\n",
            "[60]\tvalid_0's multi_logloss: 1.01034\n",
            "[70]\tvalid_0's multi_logloss: 0.998843\n",
            "[80]\tvalid_0's multi_logloss: 0.988065\n",
            "[90]\tvalid_0's multi_logloss: 0.977885\n",
            "[100]\tvalid_0's multi_logloss: 0.968338\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.968338\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:25:18,968] Trial 17 finished with value: 0.6280482423122251 and parameters: {'learning_rate': 0.004467503018101158, 'num_leaves': 102, 'max_depth': 10, 'min_child_samples': 22, 'subsample': 0.7761594257026669, 'colsample_bytree': 0.8064790131208445, 'reg_alpha': 0.2956310791092284, 'reg_lambda': 4.1058065924413234e-07}. Best is trial 16 with value: 0.692738213289999.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.820254\n",
            "[20]\tvalid_0's multi_logloss: 0.770646\n",
            "[30]\tvalid_0's multi_logloss: 0.748936\n",
            "[40]\tvalid_0's multi_logloss: 0.736436\n",
            "[50]\tvalid_0's multi_logloss: 0.727885\n",
            "[60]\tvalid_0's multi_logloss: 0.721474\n",
            "[70]\tvalid_0's multi_logloss: 0.716554\n",
            "[80]\tvalid_0's multi_logloss: 0.712754\n",
            "[90]\tvalid_0's multi_logloss: 0.709762\n",
            "[100]\tvalid_0's multi_logloss: 0.706943\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.706943\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:26:41,109] Trial 18 finished with value: 0.6895192901755351 and parameters: {'learning_rate': 0.19592927706080487, 'num_leaves': 152, 'max_depth': 7, 'min_child_samples': 41, 'subsample': 0.8067940988350515, 'colsample_bytree': 0.6064193071550553, 'reg_alpha': 0.03807467624941193, 'reg_lambda': 2.883556524898922e-07}. Best is trial 16 with value: 0.692738213289999.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.935559\n",
            "[20]\tvalid_0's multi_logloss: 0.860915\n",
            "[30]\tvalid_0's multi_logloss: 0.819747\n",
            "[40]\tvalid_0's multi_logloss: 0.794208\n",
            "[50]\tvalid_0's multi_logloss: 0.776872\n",
            "[60]\tvalid_0's multi_logloss: 0.764164\n",
            "[70]\tvalid_0's multi_logloss: 0.754431\n",
            "[80]\tvalid_0's multi_logloss: 0.746647\n",
            "[90]\tvalid_0's multi_logloss: 0.740265\n",
            "[100]\tvalid_0's multi_logloss: 0.734976\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.734976\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:29:02,791] Trial 19 finished with value: 0.6760257721521616 and parameters: {'learning_rate': 0.05786680511810161, 'num_leaves': 153, 'max_depth': 16, 'min_child_samples': 20, 'subsample': 0.6031250321323862, 'colsample_bytree': 0.9322006050177567, 'reg_alpha': 0.9603331432367691, 'reg_lambda': 0.0002339453974685718}. Best is trial 16 with value: 0.692738213289999.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 1.07595\n",
            "[20]\tvalid_0's multi_logloss: 1.05581\n",
            "[30]\tvalid_0's multi_logloss: 1.03734\n",
            "[40]\tvalid_0's multi_logloss: 1.0204\n",
            "[50]\tvalid_0's multi_logloss: 1.00479\n",
            "[60]\tvalid_0's multi_logloss: 0.990473\n",
            "[70]\tvalid_0's multi_logloss: 0.977206\n",
            "[80]\tvalid_0's multi_logloss: 0.964926\n",
            "[90]\tvalid_0's multi_logloss: 0.953487\n",
            "[100]\tvalid_0's multi_logloss: 0.942898\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.942898\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:30:58,141] Trial 20 finished with value: 0.6329389287216203 and parameters: {'learning_rate': 0.005684020041776192, 'num_leaves': 119, 'max_depth': 10, 'min_child_samples': 37, 'subsample': 0.7099077133685512, 'colsample_bytree': 0.7933939200657499, 'reg_alpha': 0.03587155773460768, 'reg_lambda': 1.7161313836493476e-08}. Best is trial 16 with value: 0.692738213289999.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.830055\n",
            "[20]\tvalid_0's multi_logloss: 0.772596\n",
            "[30]\tvalid_0's multi_logloss: 0.747638\n",
            "[40]\tvalid_0's multi_logloss: 0.733308\n",
            "[50]\tvalid_0's multi_logloss: 0.723677\n",
            "[60]\tvalid_0's multi_logloss: 0.716479\n",
            "[70]\tvalid_0's multi_logloss: 0.710971\n",
            "[80]\tvalid_0's multi_logloss: 0.706497\n",
            "[90]\tvalid_0's multi_logloss: 0.70297\n",
            "[100]\tvalid_0's multi_logloss: 0.700023\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.700023\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:33:23,935] Trial 21 finished with value: 0.6929874202407962 and parameters: {'learning_rate': 0.14892946295442894, 'num_leaves': 200, 'max_depth': 11, 'min_child_samples': 26, 'subsample': 0.7172909789705599, 'colsample_bytree': 0.9131774472721759, 'reg_alpha': 8.756226248115594, 'reg_lambda': 2.9636123400195476e-06}. Best is trial 21 with value: 0.6929874202407962.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.800156\n",
            "[20]\tvalid_0's multi_logloss: 0.752712\n",
            "[30]\tvalid_0's multi_logloss: 0.732282\n",
            "[40]\tvalid_0's multi_logloss: 0.720312\n",
            "[50]\tvalid_0's multi_logloss: 0.712231\n",
            "[60]\tvalid_0's multi_logloss: 0.706468\n",
            "[70]\tvalid_0's multi_logloss: 0.702011\n",
            "[80]\tvalid_0's multi_logloss: 0.698774\n",
            "[90]\tvalid_0's multi_logloss: 0.69608\n",
            "[100]\tvalid_0's multi_logloss: 0.693851\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.693851\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:36:02,320] Trial 22 finished with value: 0.6952458582323958 and parameters: {'learning_rate': 0.20332043574295589, 'num_leaves': 204, 'max_depth': 14, 'min_child_samples': 20, 'subsample': 0.7265932795227208, 'colsample_bytree': 0.921095547587383, 'reg_alpha': 1.5769435974963355, 'reg_lambda': 1.5054183834739337e-06}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.820141\n",
            "[20]\tvalid_0's multi_logloss: 0.765065\n",
            "[30]\tvalid_0's multi_logloss: 0.741456\n",
            "[40]\tvalid_0's multi_logloss: 0.727619\n",
            "[50]\tvalid_0's multi_logloss: 0.718582\n",
            "[60]\tvalid_0's multi_logloss: 0.711815\n",
            "[70]\tvalid_0's multi_logloss: 0.706592\n",
            "[80]\tvalid_0's multi_logloss: 0.702517\n",
            "[90]\tvalid_0's multi_logloss: 0.699409\n",
            "[100]\tvalid_0's multi_logloss: 0.696662\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.696662\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:38:45,163] Trial 23 finished with value: 0.6939686726095602 and parameters: {'learning_rate': 0.1621440835863917, 'num_leaves': 206, 'max_depth': 14, 'min_child_samples': 35, 'subsample': 0.744037851402626, 'colsample_bytree': 0.9399525760671057, 'reg_alpha': 1.1325961210027542, 'reg_lambda': 1.340005362773042e-06}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.969622\n",
            "[20]\tvalid_0's multi_logloss: 0.897782\n",
            "[30]\tvalid_0's multi_logloss: 0.853022\n",
            "[40]\tvalid_0's multi_logloss: 0.823053\n",
            "[50]\tvalid_0's multi_logloss: 0.801959\n",
            "[60]\tvalid_0's multi_logloss: 0.786322\n",
            "[70]\tvalid_0's multi_logloss: 0.774239\n",
            "[80]\tvalid_0's multi_logloss: 0.764604\n",
            "[90]\tvalid_0's multi_logloss: 0.756617\n",
            "[100]\tvalid_0's multi_logloss: 0.749898\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.749898\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:41:38,258] Trial 24 finished with value: 0.6709066460378691 and parameters: {'learning_rate': 0.039650109974753336, 'num_leaves': 209, 'max_depth': 14, 'min_child_samples': 19, 'subsample': 0.7216861314671146, 'colsample_bytree': 0.943643212134292, 'reg_alpha': 1.3907725567076736, 'reg_lambda': 1.6477327105069553e-06}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.827577\n",
            "[20]\tvalid_0's multi_logloss: 0.769983\n",
            "[30]\tvalid_0's multi_logloss: 0.745169\n",
            "[40]\tvalid_0's multi_logloss: 0.73094\n",
            "[50]\tvalid_0's multi_logloss: 0.721279\n",
            "[60]\tvalid_0's multi_logloss: 0.714353\n",
            "[70]\tvalid_0's multi_logloss: 0.708817\n",
            "[80]\tvalid_0's multi_logloss: 0.704474\n",
            "[90]\tvalid_0's multi_logloss: 0.701237\n",
            "[100]\tvalid_0's multi_logloss: 0.698369\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.698369\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-22 15:44:22,524] Trial 25 finished with value: 0.6929874202407962 and parameters: {'learning_rate': 0.1501064855531445, 'num_leaves': 206, 'max_depth': 15, 'min_child_samples': 37, 'subsample': 0.7311921980163155, 'colsample_bytree': 0.9623038251416486, 'reg_alpha': 0.07601649119429385, 'reg_lambda': 8.567038830775337e-05}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.938069\n",
            "[20]\tvalid_0's multi_logloss: 0.862254\n",
            "[30]\tvalid_0's multi_logloss: 0.819717\n",
            "[40]\tvalid_0's multi_logloss: 0.793172\n",
            "[50]\tvalid_0's multi_logloss: 0.775152\n",
            "[60]\tvalid_0's multi_logloss: 0.762087\n",
            "[70]\tvalid_0's multi_logloss: 0.751955\n",
            "[80]\tvalid_0's multi_logloss: 0.744016\n",
            "[90]\tvalid_0's multi_logloss: 0.737554\n",
            "[100]\tvalid_0's multi_logloss: 0.732011\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.732011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 15:47:13,079] Trial 26 finished with value: 0.6776092746520188 and parameters: {'learning_rate': 0.05396433856810172, 'num_leaves': 220, 'max_depth': 14, 'min_child_samples': 24, 'subsample': 0.7973813563393158, 'colsample_bytree': 0.9200770264898431, 'reg_alpha': 0.0034048321360300874, 'reg_lambda': 7.608898995951825e-08}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.89141\n",
            "[20]\tvalid_0's multi_logloss: 0.816882\n",
            "[30]\tvalid_0's multi_logloss: 0.781043\n",
            "[40]\tvalid_0's multi_logloss: 0.759966\n",
            "[50]\tvalid_0's multi_logloss: 0.745883\n",
            "[60]\tvalid_0's multi_logloss: 0.735724\n",
            "[70]\tvalid_0's multi_logloss: 0.727979\n",
            "[80]\tvalid_0's multi_logloss: 0.721711\n",
            "[90]\tvalid_0's multi_logloss: 0.716641\n",
            "[100]\tvalid_0's multi_logloss: 0.712322\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.712322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 15:50:04,338] Trial 27 finished with value: 0.6867001365446418 and parameters: {'learning_rate': 0.08094249209380999, 'num_leaves': 256, 'max_depth': 13, 'min_child_samples': 100, 'subsample': 0.8286006275988708, 'colsample_bytree': 0.848248716325895, 'reg_alpha': 2.2739753435433414e-05, 'reg_lambda': 2.1102873791150453e-06}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.82148\n",
            "[20]\tvalid_0's multi_logloss: 0.767524\n",
            "[30]\tvalid_0's multi_logloss: 0.744099\n",
            "[40]\tvalid_0's multi_logloss: 0.730617\n",
            "[50]\tvalid_0's multi_logloss: 0.721196\n",
            "[60]\tvalid_0's multi_logloss: 0.714284\n",
            "[70]\tvalid_0's multi_logloss: 0.709205\n",
            "[80]\tvalid_0's multi_logloss: 0.704999\n",
            "[90]\tvalid_0's multi_logloss: 0.701501\n",
            "[100]\tvalid_0's multi_logloss: 0.698748\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.698748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 15:52:22,197] Trial 28 finished with value: 0.6932522026260183 and parameters: {'learning_rate': 0.1681492167986351, 'num_leaves': 162, 'max_depth': 15, 'min_child_samples': 48, 'subsample': 0.6250399434825185, 'colsample_bytree': 0.9071753212796285, 'reg_alpha': 2.1324875473201956, 'reg_lambda': 3.736664333365487e-05}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.784144\n",
            "[20]\tvalid_0's multi_logloss: 0.744036\n",
            "[30]\tvalid_0's multi_logloss: 0.726629\n",
            "[40]\tvalid_0's multi_logloss: 0.716476\n",
            "[50]\tvalid_0's multi_logloss: 0.709746\n",
            "[60]\tvalid_0's multi_logloss: 0.70504\n",
            "[70]\tvalid_0's multi_logloss: 0.701808\n",
            "[80]\tvalid_0's multi_logloss: 0.69969\n",
            "[90]\tvalid_0's multi_logloss: 0.698691\n",
            "[100]\tvalid_0's multi_logloss: 0.698061\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.698061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 15:54:35,911] Trial 29 finished with value: 0.6928108986506482 and parameters: {'learning_rate': 0.2656633737871981, 'num_leaves': 166, 'max_depth': 15, 'min_child_samples': 48, 'subsample': 0.6283161856037432, 'colsample_bytree': 0.9855842880218362, 'reg_alpha': 0.10896667235410508, 'reg_lambda': 0.001597067098638028}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 1.09438\n",
            "[20]\tvalid_0's multi_logloss: 1.09047\n",
            "[30]\tvalid_0's multi_logloss: 1.08664\n",
            "[40]\tvalid_0's multi_logloss: 1.08289\n",
            "[50]\tvalid_0's multi_logloss: 1.0792\n",
            "[60]\tvalid_0's multi_logloss: 1.0756\n",
            "[70]\tvalid_0's multi_logloss: 1.07205\n",
            "[80]\tvalid_0's multi_logloss: 1.06856\n",
            "[90]\tvalid_0's multi_logloss: 1.06514\n",
            "[100]\tvalid_0's multi_logloss: 1.06177\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 1.06177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 15:55:57,563] Trial 30 finished with value: 0.583964571078495 and parameters: {'learning_rate': 0.0010434217237111916, 'num_leaves': 52, 'max_depth': 15, 'min_child_samples': 49, 'subsample': 0.5869591740212716, 'colsample_bytree': 0.9567731529742661, 'reg_alpha': 0.006216523645037096, 'reg_lambda': 5.0982431816993344e-05}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.777215\n",
            "[20]\tvalid_0's multi_logloss: 0.73958\n",
            "[30]\tvalid_0's multi_logloss: 0.723567\n",
            "[40]\tvalid_0's multi_logloss: 0.714262\n",
            "[50]\tvalid_0's multi_logloss: 0.707985\n",
            "[60]\tvalid_0's multi_logloss: 0.703565\n",
            "[70]\tvalid_0's multi_logloss: 0.700681\n",
            "[80]\tvalid_0's multi_logloss: 0.69859\n",
            "[90]\tvalid_0's multi_logloss: 0.69729\n",
            "[100]\tvalid_0's multi_logloss: 0.696104\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.696104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 15:58:19,789] Trial 31 finished with value: 0.69398943985546 and parameters: {'learning_rate': 0.28381634451817656, 'num_leaves': 194, 'max_depth': 13, 'min_child_samples': 38, 'subsample': 0.7534545160492495, 'colsample_bytree': 0.9201005098324443, 'reg_alpha': 2.424747209528658, 'reg_lambda': 5.8476521121932945e-08}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.776505\n",
            "[20]\tvalid_0's multi_logloss: 0.739434\n",
            "[30]\tvalid_0's multi_logloss: 0.723733\n",
            "[40]\tvalid_0's multi_logloss: 0.714301\n",
            "[50]\tvalid_0's multi_logloss: 0.70854\n",
            "[60]\tvalid_0's multi_logloss: 0.704515\n",
            "[70]\tvalid_0's multi_logloss: 0.701512\n",
            "[80]\tvalid_0's multi_logloss: 0.699584\n",
            "[90]\tvalid_0's multi_logloss: 0.698488\n",
            "[100]\tvalid_0's multi_logloss: 0.69778\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.69778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:00:24,764] Trial 32 finished with value: 0.6931120237161948 and parameters: {'learning_rate': 0.2977277085290891, 'num_leaves': 165, 'max_depth': 13, 'min_child_samples': 38, 'subsample': 0.756594943744406, 'colsample_bytree': 0.8382706080671114, 'reg_alpha': 1.3172142296404754, 'reg_lambda': 6.200929768565587e-08}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.804811\n",
            "[20]\tvalid_0's multi_logloss: 0.755798\n",
            "[30]\tvalid_0's multi_logloss: 0.734818\n",
            "[40]\tvalid_0's multi_logloss: 0.722591\n",
            "[50]\tvalid_0's multi_logloss: 0.714309\n",
            "[60]\tvalid_0's multi_logloss: 0.708253\n",
            "[70]\tvalid_0's multi_logloss: 0.703428\n",
            "[80]\tvalid_0's multi_logloss: 0.699879\n",
            "[90]\tvalid_0's multi_logloss: 0.697091\n",
            "[100]\tvalid_0's multi_logloss: 0.69459\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.69459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:02:57,120] Trial 33 finished with value: 0.6945034291914792 and parameters: {'learning_rate': 0.19502970459077415, 'num_leaves': 195, 'max_depth': 16, 'min_child_samples': 44, 'subsample': 0.74951421350585, 'colsample_bytree': 0.9007823178836107, 'reg_alpha': 1.9530048571348764, 'reg_lambda': 9.819786826253725e-07}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.876051\n",
            "[20]\tvalid_0's multi_logloss: 0.805912\n",
            "[30]\tvalid_0's multi_logloss: 0.773304\n",
            "[40]\tvalid_0's multi_logloss: 0.754332\n",
            "[50]\tvalid_0's multi_logloss: 0.741553\n",
            "[60]\tvalid_0's multi_logloss: 0.73211\n",
            "[70]\tvalid_0's multi_logloss: 0.725016\n",
            "[80]\tvalid_0's multi_logloss: 0.719247\n",
            "[90]\tvalid_0's multi_logloss: 0.7143\n",
            "[100]\tvalid_0's multi_logloss: 0.710176\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.710176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:05:32,011] Trial 34 finished with value: 0.6876969643478306 and parameters: {'learning_rate': 0.09646727173268844, 'num_leaves': 194, 'max_depth': 16, 'min_child_samples': 34, 'subsample': 0.750021887925773, 'colsample_bytree': 0.8798408234028349, 'reg_alpha': 2.5912320574714665, 'reg_lambda': 4.5670911859614597e-08}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.79723\n",
            "[20]\tvalid_0's multi_logloss: 0.750543\n",
            "[30]\tvalid_0's multi_logloss: 0.731058\n",
            "[40]\tvalid_0's multi_logloss: 0.71949\n",
            "[50]\tvalid_0's multi_logloss: 0.71183\n",
            "[60]\tvalid_0's multi_logloss: 0.706334\n",
            "[70]\tvalid_0's multi_logloss: 0.702363\n",
            "[80]\tvalid_0's multi_logloss: 0.699189\n",
            "[90]\tvalid_0's multi_logloss: 0.69674\n",
            "[100]\tvalid_0's multi_logloss: 0.695008\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.695008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:08:00,202] Trial 35 finished with value: 0.6941088515193836 and parameters: {'learning_rate': 0.20866068962595444, 'num_leaves': 222, 'max_depth': 14, 'min_child_samples': 59, 'subsample': 0.6701173787549743, 'colsample_bytree': 0.7735543123037549, 'reg_alpha': 0.11625241537761884, 'reg_lambda': 8.841898814739074e-07}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.99744\n",
            "[20]\tvalid_0's multi_logloss: 0.932183\n",
            "[30]\tvalid_0's multi_logloss: 0.886798\n",
            "[40]\tvalid_0's multi_logloss: 0.854063\n",
            "[50]\tvalid_0's multi_logloss: 0.829975\n",
            "[60]\tvalid_0's multi_logloss: 0.811531\n",
            "[70]\tvalid_0's multi_logloss: 0.797179\n",
            "[80]\tvalid_0's multi_logloss: 0.785649\n",
            "[90]\tvalid_0's multi_logloss: 0.776142\n",
            "[100]\tvalid_0's multi_logloss: 0.76819\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.76819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:10:32,627] Trial 36 finished with value: 0.6664105373005695 and parameters: {'learning_rate': 0.029147047927054682, 'num_leaves': 242, 'max_depth': 13, 'min_child_samples': 58, 'subsample': 0.8016330384961625, 'colsample_bytree': 0.6282130198268223, 'reg_alpha': 0.002052818609222354, 'reg_lambda': 1.355683829491551e-07}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.794394\n",
            "[20]\tvalid_0's multi_logloss: 0.74854\n",
            "[30]\tvalid_0's multi_logloss: 0.729448\n",
            "[40]\tvalid_0's multi_logloss: 0.718396\n",
            "[50]\tvalid_0's multi_logloss: 0.710859\n",
            "[60]\tvalid_0's multi_logloss: 0.70544\n",
            "[70]\tvalid_0's multi_logloss: 0.701585\n",
            "[80]\tvalid_0's multi_logloss: 0.698356\n",
            "[90]\tvalid_0's multi_logloss: 0.695846\n",
            "[100]\tvalid_0's multi_logloss: 0.694381\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.694381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:12:56,768] Trial 37 finished with value: 0.6945916899865532 and parameters: {'learning_rate': 0.21669419605771448, 'num_leaves': 220, 'max_depth': 16, 'min_child_samples': 75, 'subsample': 0.6704220051869529, 'colsample_bytree': 0.7755543147553102, 'reg_alpha': 5.159391855166696e-05, 'reg_lambda': 7.040568919161277e-07}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 1.05488\n",
            "[20]\tvalid_0's multi_logloss: 1.01893\n",
            "[30]\tvalid_0's multi_logloss: 0.988239\n",
            "[40]\tvalid_0's multi_logloss: 0.96182\n",
            "[50]\tvalid_0's multi_logloss: 0.939023\n",
            "[60]\tvalid_0's multi_logloss: 0.919227\n",
            "[70]\tvalid_0's multi_logloss: 0.90193\n",
            "[80]\tvalid_0's multi_logloss: 0.886793\n",
            "[90]\tvalid_0's multi_logloss: 0.873352\n",
            "[100]\tvalid_0's multi_logloss: 0.861558\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.861558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:15:27,017] Trial 38 finished with value: 0.6495994517447082 and parameters: {'learning_rate': 0.011025337379325307, 'num_leaves': 222, 'max_depth': 16, 'min_child_samples': 73, 'subsample': 0.6679490827668636, 'colsample_bytree': 0.6728010419935233, 'reg_alpha': 4.2929313559727454e-05, 'reg_lambda': 7.042135518863523e-07}. Best is trial 22 with value: 0.6952458582323958.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.789286\n",
            "[20]\tvalid_0's multi_logloss: 0.74511\n",
            "[30]\tvalid_0's multi_logloss: 0.726656\n",
            "[40]\tvalid_0's multi_logloss: 0.715881\n",
            "[50]\tvalid_0's multi_logloss: 0.708948\n",
            "[60]\tvalid_0's multi_logloss: 0.704046\n",
            "[70]\tvalid_0's multi_logloss: 0.700665\n",
            "[80]\tvalid_0's multi_logloss: 0.697788\n",
            "[90]\tvalid_0's multi_logloss: 0.695497\n",
            "[100]\tvalid_0's multi_logloss: 0.694359\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.694359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:17:57,859] Trial 39 finished with value: 0.6952562418553457 and parameters: {'learning_rate': 0.2276235111540587, 'num_leaves': 242, 'max_depth': 15, 'min_child_samples': 76, 'subsample': 0.6919191976188034, 'colsample_bytree': 0.7695346874835, 'reg_alpha': 1.3446566777159403e-06, 'reg_lambda': 0.010913349156401042}. Best is trial 39 with value: 0.6952562418553457.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.926401\n",
            "[20]\tvalid_0's multi_logloss: 0.849462\n",
            "[30]\tvalid_0's multi_logloss: 0.807943\n",
            "[40]\tvalid_0's multi_logloss: 0.782759\n",
            "[50]\tvalid_0's multi_logloss: 0.765999\n",
            "[60]\tvalid_0's multi_logloss: 0.753626\n",
            "[70]\tvalid_0's multi_logloss: 0.744268\n",
            "[80]\tvalid_0's multi_logloss: 0.736801\n",
            "[90]\tvalid_0's multi_logloss: 0.730576\n",
            "[100]\tvalid_0's multi_logloss: 0.725433\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.725433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:20:43,650] Trial 40 finished with value: 0.6816744630368982 and parameters: {'learning_rate': 0.05985416516742102, 'num_leaves': 249, 'max_depth': 15, 'min_child_samples': 83, 'subsample': 0.6978768530144364, 'colsample_bytree': 0.7616570119036863, 'reg_alpha': 1.1774778147724144e-06, 'reg_lambda': 0.008201441176931855}. Best is trial 39 with value: 0.6952562418553457.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.796554\n",
            "[20]\tvalid_0's multi_logloss: 0.75005\n",
            "[30]\tvalid_0's multi_logloss: 0.730373\n",
            "[40]\tvalid_0's multi_logloss: 0.71901\n",
            "[50]\tvalid_0's multi_logloss: 0.711462\n",
            "[60]\tvalid_0's multi_logloss: 0.705967\n",
            "[70]\tvalid_0's multi_logloss: 0.701927\n",
            "[80]\tvalid_0's multi_logloss: 0.698824\n",
            "[90]\tvalid_0's multi_logloss: 0.69638\n",
            "[100]\tvalid_0's multi_logloss: 0.694583\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.694583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:23:10,634] Trial 41 finished with value: 0.6953756535192694 and parameters: {'learning_rate': 0.21062203597149756, 'num_leaves': 222, 'max_depth': 16, 'min_child_samples': 77, 'subsample': 0.6580912408833793, 'colsample_bytree': 0.7692238505371277, 'reg_alpha': 5.901408442631763e-08, 'reg_lambda': 0.0021466669288140797}. Best is trial 41 with value: 0.6953756535192694.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.798247\n",
            "[20]\tvalid_0's multi_logloss: 0.751031\n",
            "[30]\tvalid_0's multi_logloss: 0.73123\n",
            "[40]\tvalid_0's multi_logloss: 0.719661\n",
            "[50]\tvalid_0's multi_logloss: 0.711859\n",
            "[60]\tvalid_0's multi_logloss: 0.706556\n",
            "[70]\tvalid_0's multi_logloss: 0.702351\n",
            "[80]\tvalid_0's multi_logloss: 0.698866\n",
            "[90]\tvalid_0's multi_logloss: 0.696581\n",
            "[100]\tvalid_0's multi_logloss: 0.694923\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.694923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:25:49,757] Trial 42 finished with value: 0.6942957567324816 and parameters: {'learning_rate': 0.20501497237523228, 'num_leaves': 233, 'max_depth': 16, 'min_child_samples': 79, 'subsample': 0.643655222514944, 'colsample_bytree': 0.7266044030685062, 'reg_alpha': 4.200445235994374e-08, 'reg_lambda': 0.03219101292858958}. Best is trial 41 with value: 0.6953756535192694.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.851381\n",
            "[20]\tvalid_0's multi_logloss: 0.785368\n",
            "[30]\tvalid_0's multi_logloss: 0.756424\n",
            "[40]\tvalid_0's multi_logloss: 0.739795\n",
            "[50]\tvalid_0's multi_logloss: 0.728631\n",
            "[60]\tvalid_0's multi_logloss: 0.720433\n",
            "[70]\tvalid_0's multi_logloss: 0.714118\n",
            "[80]\tvalid_0's multi_logloss: 0.709199\n",
            "[90]\tvalid_0's multi_logloss: 0.704831\n",
            "[100]\tvalid_0's multi_logloss: 0.701297\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.701297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:28:36,401] Trial 43 finished with value: 0.6925201572080515 and parameters: {'learning_rate': 0.11751946001260129, 'num_leaves': 244, 'max_depth': 16, 'min_child_samples': 93, 'subsample': 0.5774299188634813, 'colsample_bytree': 0.6903109997304735, 'reg_alpha': 1.416041997519428e-07, 'reg_lambda': 0.004113161722515314}. Best is trial 41 with value: 0.6953756535192694.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.789555\n",
            "[20]\tvalid_0's multi_logloss: 0.745986\n",
            "[30]\tvalid_0's multi_logloss: 0.727743\n",
            "[40]\tvalid_0's multi_logloss: 0.717105\n",
            "[50]\tvalid_0's multi_logloss: 0.710247\n",
            "[60]\tvalid_0's multi_logloss: 0.705193\n",
            "[70]\tvalid_0's multi_logloss: 0.701696\n",
            "[80]\tvalid_0's multi_logloss: 0.698806\n",
            "[90]\tvalid_0's multi_logloss: 0.696549\n",
            "[100]\tvalid_0's multi_logloss: 0.69528\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.69528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:31:04,659] Trial 44 finished with value: 0.69488243142915 and parameters: {'learning_rate': 0.2324130421867477, 'num_leaves': 215, 'max_depth': 15, 'min_child_samples': 66, 'subsample': 0.9913735116297412, 'colsample_bytree': 0.7229164524299625, 'reg_alpha': 1.7526871585297524e-06, 'reg_lambda': 0.0008497191372820878}. Best is trial 41 with value: 0.6953756535192694.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.79154\n",
            "[20]\tvalid_0's multi_logloss: 0.747472\n",
            "[30]\tvalid_0's multi_logloss: 0.728795\n",
            "[40]\tvalid_0's multi_logloss: 0.717964\n",
            "[50]\tvalid_0's multi_logloss: 0.710663\n",
            "[60]\tvalid_0's multi_logloss: 0.705668\n",
            "[70]\tvalid_0's multi_logloss: 0.701958\n",
            "[80]\tvalid_0's multi_logloss: 0.699158\n",
            "[90]\tvalid_0's multi_logloss: 0.697009\n",
            "[100]\tvalid_0's multi_logloss: 0.695546\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.695546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:33:26,291] Trial 45 finished with value: 0.6941503860111832 and parameters: {'learning_rate': 0.22518173519489812, 'num_leaves': 215, 'max_depth': 15, 'min_child_samples': 66, 'subsample': 0.9911590078906589, 'colsample_bytree': 0.728154761062549, 'reg_alpha': 3.1834331784950308e-06, 'reg_lambda': 0.045075835718831204}. Best is trial 41 with value: 0.6953756535192694.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.871609\n",
            "[20]\tvalid_0's multi_logloss: 0.800947\n",
            "[30]\tvalid_0's multi_logloss: 0.768838\n",
            "[40]\tvalid_0's multi_logloss: 0.750137\n",
            "[50]\tvalid_0's multi_logloss: 0.73761\n",
            "[60]\tvalid_0's multi_logloss: 0.728567\n",
            "[70]\tvalid_0's multi_logloss: 0.721703\n",
            "[80]\tvalid_0's multi_logloss: 0.716054\n",
            "[90]\tvalid_0's multi_logloss: 0.711379\n",
            "[100]\tvalid_0's multi_logloss: 0.707553\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.707553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:35:50,881] Trial 46 finished with value: 0.689358344019812 and parameters: {'learning_rate': 0.09925043535179144, 'num_leaves': 232, 'max_depth': 14, 'min_child_samples': 72, 'subsample': 0.8902848621247501, 'colsample_bytree': 0.6259751137097704, 'reg_alpha': 3.874105984992348e-05, 'reg_lambda': 0.0011810957044216864}. Best is trial 41 with value: 0.6953756535192694.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.825029\n",
            "[20]\tvalid_0's multi_logloss: 0.779362\n",
            "[30]\tvalid_0's multi_logloss: 0.759351\n",
            "[40]\tvalid_0's multi_logloss: 0.74732\n",
            "[50]\tvalid_0's multi_logloss: 0.738452\n",
            "[60]\tvalid_0's multi_logloss: 0.732032\n",
            "[70]\tvalid_0's multi_logloss: 0.726878\n",
            "[80]\tvalid_0's multi_logloss: 0.722733\n",
            "[90]\tvalid_0's multi_logloss: 0.719293\n",
            "[100]\tvalid_0's multi_logloss: 0.71639\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.71639\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:36:57,423] Trial 47 finished with value: 0.682936073225309 and parameters: {'learning_rate': 0.24154971452914184, 'num_leaves': 31, 'max_depth': 15, 'min_child_samples': 64, 'subsample': 0.6816183790461292, 'colsample_bytree': 0.7802754852885572, 'reg_alpha': 3.6332693482673153e-07, 'reg_lambda': 0.000623343294588864}. Best is trial 41 with value: 0.6953756535192694.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.838512\n",
            "[20]\tvalid_0's multi_logloss: 0.776996\n",
            "[30]\tvalid_0's multi_logloss: 0.750311\n",
            "[40]\tvalid_0's multi_logloss: 0.735032\n",
            "[50]\tvalid_0's multi_logloss: 0.724851\n",
            "[60]\tvalid_0's multi_logloss: 0.717324\n",
            "[70]\tvalid_0's multi_logloss: 0.711391\n",
            "[80]\tvalid_0's multi_logloss: 0.706704\n",
            "[90]\tvalid_0's multi_logloss: 0.702829\n",
            "[100]\tvalid_0's multi_logloss: 0.699703\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.699703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:39:24,245] Trial 48 finished with value: 0.6931899008883189 and parameters: {'learning_rate': 0.13453638966299902, 'num_leaves': 224, 'max_depth': 14, 'min_child_samples': 80, 'subsample': 0.6477607347218769, 'colsample_bytree': 0.7121131519039022, 'reg_alpha': 9.586184155172608e-06, 'reg_lambda': 0.02393163876880972}. Best is trial 41 with value: 0.6953756535192694.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.906501\n",
            "[20]\tvalid_0's multi_logloss: 0.830254\n",
            "[30]\tvalid_0's multi_logloss: 0.791965\n",
            "[40]\tvalid_0's multi_logloss: 0.769216\n",
            "[50]\tvalid_0's multi_logloss: 0.754089\n",
            "[60]\tvalid_0's multi_logloss: 0.743103\n",
            "[70]\tvalid_0's multi_logloss: 0.734612\n",
            "[80]\tvalid_0's multi_logloss: 0.72792\n",
            "[90]\tvalid_0's multi_logloss: 0.722334\n",
            "[100]\tvalid_0's multi_logloss: 0.71763\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.71763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:42:07,399] Trial 49 finished with value: 0.6842080670366698 and parameters: {'learning_rate': 0.07167478158522553, 'num_leaves': 244, 'max_depth': 15, 'min_child_samples': 76, 'subsample': 0.5572078701737916, 'colsample_bytree': 0.7492229241104136, 'reg_alpha': 0.00011602969017084999, 'reg_lambda': 0.0001564256909356638}. Best is trial 41 with value: 0.6953756535192694.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Best LightGBM params: {'learning_rate': 0.21062203597149756, 'num_leaves': 222, 'max_depth': 16, 'min_child_samples': 77, 'subsample': 0.6580912408833793, 'colsample_bytree': 0.7692238505371277, 'reg_alpha': 5.901408442631763e-08, 'reg_lambda': 0.0021466669288140797}\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[10]\tvalid_0's multi_logloss: 0.796554\n",
            "[20]\tvalid_0's multi_logloss: 0.75005\n",
            "[30]\tvalid_0's multi_logloss: 0.730373\n",
            "[40]\tvalid_0's multi_logloss: 0.719009\n",
            "[50]\tvalid_0's multi_logloss: 0.711462\n",
            "[60]\tvalid_0's multi_logloss: 0.705967\n",
            "[70]\tvalid_0's multi_logloss: 0.701927\n",
            "[80]\tvalid_0's multi_logloss: 0.698823\n",
            "[90]\tvalid_0's multi_logloss: 0.696379\n",
            "[100]\tvalid_0's multi_logloss: 0.694583\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\tvalid_0's multi_logloss: 0.694583\n",
            "✅ LightGBM model saved to best_lightgbm.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xgboost hyper-parameter tuning"
      ],
      "metadata": {
        "id": "SmGdkc8gDXzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBOOST OPTUNA TUNING + SAVE\n",
        "# -------------------------------\n",
        "def objective_xgb(trial):\n",
        "    param = {\n",
        "        'objective': 'multi:softprob',\n",
        "        'eval_metric': 'mlogloss',\n",
        "        'num_class': len(set(y)),\n",
        "        'tree_method': 'gpu_hist',\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
        "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True)\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(\n",
        "        **param,\n",
        "        use_label_encoder=False,\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    model.set_params(early_stopping_rounds=50)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    preds = model.predict(X_val)\n",
        "    return accuracy_score(y_val, preds)\n",
        "\n",
        "# Run XGBoost tuning\n",
        "study_xgb = optuna.create_study(direction='maximize')\n",
        "study_xgb.optimize(objective_xgb, n_trials=50)\n",
        "print(\"✅ Best XGBoost params:\", study_xgb.best_params)\n",
        "\n",
        "# Train final XGBoost model with best params\n",
        "best_params_xgb = study_xgb.best_params\n",
        "best_params_xgb.update({\n",
        "    'objective': 'multi:softprob',\n",
        "    'eval_metric': 'mlogloss',\n",
        "    'num_class': len(set(y)),\n",
        "    'tree_method': 'gpu_hist',\n",
        "    'use_label_encoder': False,\n",
        "    'random_state': 42\n",
        "})\n",
        "\n",
        "final_xgb = xgb.XGBClassifier(**best_params_xgb)\n",
        "final_xgb.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    early_stopping_rounds=50,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Save final XGBoost model\n",
        "joblib.dump(final_xgb, 'best_xgboost.pkl')\n",
        "print(\"✅ XGBoost model saved to best_xgboost.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cwqKupKtVIz",
        "outputId": "01e1c279-acaa-4a41-aa7e-642f673fbe7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-22 16:47:33,663] A new study created in memory with name: no-name-85b8f5bc-753c-40e6-93ec-d825c9560a3a\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:48:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:48:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:48:20] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:48:20] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 16:48:21,071] Trial 0 finished with value: 0.6809476094304063 and parameters: {'learning_rate': 0.2101417895389979, 'max_depth': 6, 'min_child_weight': 10, 'gamma': 0.3492477011848105, 'subsample': 0.8113578035102271, 'colsample_bytree': 0.5073264951620932, 'lambda': 3.184371978183506, 'alpha': 1.0645794603748973}. Best is trial 0 with value: 0.6809476094304063.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:48:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:48:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:49:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 16:49:23,927] Trial 1 finished with value: 0.6740996100949582 and parameters: {'learning_rate': 0.03857076776549258, 'max_depth': 10, 'min_child_weight': 10, 'gamma': 0.6491181539031016, 'subsample': 0.6845373783326664, 'colsample_bytree': 0.5007456389605647, 'lambda': 5.0901458972428956e-08, 'alpha': 1.4390464761308467e-05}. Best is trial 0 with value: 0.6809476094304063.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:49:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:49:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:50:32] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 16:50:33,493] Trial 2 finished with value: 0.6968605116011027 and parameters: {'learning_rate': 0.1784862493581227, 'max_depth': 12, 'min_child_weight': 5, 'gamma': 2.3979278404903788, 'subsample': 0.9489109573312251, 'colsample_bytree': 0.5103137663978106, 'lambda': 4.030386349553324e-08, 'alpha': 6.061639688547507}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:50:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:50:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:51:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 16:51:11,843] Trial 3 finished with value: 0.6150998644937204 and parameters: {'learning_rate': 0.006949462359043251, 'max_depth': 5, 'min_child_weight': 7, 'gamma': 4.413415179170274, 'subsample': 0.6402549365261112, 'colsample_bytree': 0.6905196517005381, 'lambda': 0.5784051156979578, 'alpha': 2.733435258223497e-07}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:51:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:51:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:52:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 16:52:01,736] Trial 4 finished with value: 0.6935533276915649 and parameters: {'learning_rate': 0.26247979178337877, 'max_depth': 9, 'min_child_weight': 7, 'gamma': 3.8770226156600063, 'subsample': 0.7337567964850344, 'colsample_bytree': 0.6825817104249783, 'lambda': 1.0783456580544034e-06, 'alpha': 0.2545757027645302}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:52:27] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:52:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:52:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 16:52:38,122] Trial 5 finished with value: 0.577370970505319 and parameters: {'learning_rate': 0.0018878861933517128, 'max_depth': 4, 'min_child_weight': 3, 'gamma': 0.6659398123984006, 'subsample': 0.5415786137812836, 'colsample_bytree': 0.9971046306084119, 'lambda': 4.5772127807297797e-08, 'alpha': 1.5623636464574604e-07}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:53:03] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:53:03] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:53:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 16:53:46,995] Trial 6 finished with value: 0.6407058786881331 and parameters: {'learning_rate': 0.0010468220920465986, 'max_depth': 10, 'min_child_weight': 8, 'gamma': 3.9953940339714955, 'subsample': 0.5925103472346278, 'colsample_bytree': 0.9492945081493974, 'lambda': 1.943869271702813e-08, 'alpha': 4.101304769884594}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:54:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:54:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:54:34] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 16:54:35,113] Trial 7 finished with value: 0.6782115247831121 and parameters: {'learning_rate': 0.08786630285632757, 'max_depth': 8, 'min_child_weight': 7, 'gamma': 0.08263327051968306, 'subsample': 0.5920642901955236, 'colsample_bytree': 0.7981889978067735, 'lambda': 0.44113877640260496, 'alpha': 0.041528016298387475}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:55:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:55:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:55:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 16:55:38,123] Trial 8 finished with value: 0.6506378140396966 and parameters: {'learning_rate': 0.0013252015640621538, 'max_depth': 10, 'min_child_weight': 9, 'gamma': 3.8503858091804215, 'subsample': 0.6621391251286906, 'colsample_bytree': 0.5592544368995076, 'lambda': 1.560222025131331e-08, 'alpha': 8.02790119295672e-06}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:56:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:56:04] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:56:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 16:56:58,761] Trial 9 finished with value: 0.6804595791517618 and parameters: {'learning_rate': 0.04431091859685443, 'max_depth': 11, 'min_child_weight': 7, 'gamma': 1.3730477286652114, 'subsample': 0.9751487096500329, 'colsample_bytree': 0.5826315514272238, 'lambda': 9.336163731099356e-06, 'alpha': 0.3163239628518279}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:57:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [16:57:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:01:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:01:13,119] Trial 10 finished with value: 0.6724486140459268 and parameters: {'learning_rate': 0.009214318043072984, 'max_depth': 14, 'min_child_weight': 3, 'gamma': 2.336052469848319, 'subsample': 0.9957256935590046, 'colsample_bytree': 0.828480696682363, 'lambda': 0.0015010827576773652, 'alpha': 0.0028426501418903683}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:01:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:01:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:02:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:02:25,798] Trial 11 finished with value: 0.6888547383067426 and parameters: {'learning_rate': 0.2864069130098649, 'max_depth': 14, 'min_child_weight': 5, 'gamma': 2.94037202358737, 'subsample': 0.7965034561133874, 'colsample_bytree': 0.6674296705510786, 'lambda': 3.730865066262091e-06, 'alpha': 0.00887134513549435}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:02:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:02:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:03:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:03:41,410] Trial 12 finished with value: 0.6926136098146004 and parameters: {'learning_rate': 0.11491493106847593, 'max_depth': 12, 'min_child_weight': 5, 'gamma': 2.8140336795254424, 'subsample': 0.8750059989757142, 'colsample_bytree': 0.6756998474225315, 'lambda': 3.1437789203288483e-06, 'alpha': 7.141725772012258}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:04:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:04:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:04:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:04:24,276] Trial 13 finished with value: 0.6755481254964669 and parameters: {'learning_rate': 0.09411428136429419, 'max_depth': 7, 'min_child_weight': 1, 'gamma': 1.9264519959234212, 'subsample': 0.8909315451922182, 'colsample_bytree': 0.6157018804115167, 'lambda': 0.0002056930292529847, 'alpha': 0.09956588666727423}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:04:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:04:50] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:06:10] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:06:11,552] Trial 14 finished with value: 0.6781647984798376 and parameters: {'learning_rate': 0.031196269311099815, 'max_depth': 12, 'min_child_weight': 4, 'gamma': 3.3708909106320655, 'subsample': 0.7359918646368451, 'colsample_bytree': 0.7529237823371789, 'lambda': 6.42852113844385e-07, 'alpha': 0.0004164229069054725}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:06:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:06:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:07:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:07:09,500] Trial 15 finished with value: 0.6924838145277269 and parameters: {'learning_rate': 0.29922570379255614, 'max_depth': 15, 'min_child_weight': 6, 'gamma': 4.860002346287511, 'subsample': 0.9117109325079717, 'colsample_bytree': 0.8593275845333588, 'lambda': 5.0650303679527655e-05, 'alpha': 7.4593668480814905}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:07:35] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:07:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:07:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:07:56,392] Trial 16 finished with value: 0.6861705717741977 and parameters: {'learning_rate': 0.14338146848764224, 'max_depth': 8, 'min_child_weight': 1, 'gamma': 1.751422667356344, 'subsample': 0.7502057647806609, 'colsample_bytree': 0.6249650438371517, 'lambda': 0.0028491571575681154, 'alpha': 0.22086383962265713}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:08:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:08:22] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:10:20] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:10:21,701] Trial 17 finished with value: 0.6740684592261086 and parameters: {'learning_rate': 0.016924751655620945, 'max_depth': 13, 'min_child_weight': 6, 'gamma': 3.2216184932641205, 'subsample': 0.830320092297012, 'colsample_bytree': 0.7328454781209438, 'lambda': 4.3698338882666926e-07, 'alpha': 0.009005914105787731}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:10:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:10:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:11:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:11:15,150] Trial 18 finished with value: 0.6768097356848778 and parameters: {'learning_rate': 0.06043308937272888, 'max_depth': 9, 'min_child_weight': 4, 'gamma': 2.4113166424630768, 'subsample': 0.7309149008587839, 'colsample_bytree': 0.5552047366845152, 'lambda': 2.9104853027646294e-07, 'alpha': 0.0007832600597571823}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:11:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:11:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:11:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:11:50,641] Trial 19 finished with value: 0.6067514316420142 and parameters: {'learning_rate': 0.020266438804808266, 'max_depth': 3, 'min_child_weight': 8, 'gamma': 3.7038073372224884, 'subsample': 0.925242726096334, 'colsample_bytree': 0.8768473227647445, 'lambda': 2.7572075619114238e-05, 'alpha': 8.113508795615735e-05}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:12:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:12:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:13:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:13:01,625] Trial 20 finished with value: 0.6949343495438993 and parameters: {'learning_rate': 0.1522084947253859, 'max_depth': 12, 'min_child_weight': 3, 'gamma': 4.561862881687084, 'subsample': 0.7746040221915834, 'colsample_bytree': 0.7524579505026654, 'lambda': 0.03169101037508916, 'alpha': 1.0267324358716134}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:13:27] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:13:27] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:14:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:14:05,845] Trial 21 finished with value: 0.6938596445685864 and parameters: {'learning_rate': 0.19416725932647452, 'max_depth': 12, 'min_child_weight': 3, 'gamma': 4.833681989880454, 'subsample': 0.7715964027774945, 'colsample_bytree': 0.7414350443576818, 'lambda': 0.04791716987363395, 'alpha': 0.7096121530845948}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:14:31] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:14:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:15:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:15:15,996] Trial 22 finished with value: 0.6937506165276126 and parameters: {'learning_rate': 0.15000048577045397, 'max_depth': 12, 'min_child_weight': 3, 'gamma': 4.712990722581235, 'subsample': 0.8397889518828398, 'colsample_bytree': 0.7643684270133156, 'lambda': 0.02029353656181596, 'alpha': 0.9198295605868599}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:15:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:15:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:16:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:16:34,827] Trial 23 finished with value: 0.6934806423309157 and parameters: {'learning_rate': 0.17252512642245146, 'max_depth': 13, 'min_child_weight': 2, 'gamma': 4.3207905709405665, 'subsample': 0.7804009245549235, 'colsample_bytree': 0.9190216028044875, 'lambda': 0.03803061435252158, 'alpha': 0.02367563745846165}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:17:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:17:00] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:18:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:18:17,171] Trial 24 finished with value: 0.6901059648722035 and parameters: {'learning_rate': 0.07398266042238649, 'max_depth': 15, 'min_child_weight': 4, 'gamma': 4.919847745934968, 'subsample': 0.9416525212795718, 'colsample_bytree': 0.7951168978472333, 'lambda': 0.021615279161114408, 'alpha': 1.5954147774352858}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:18:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:18:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:19:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:19:43,453] Trial 25 finished with value: 0.6863574769872957 and parameters: {'learning_rate': 0.061953009014589625, 'max_depth': 11, 'min_child_weight': 2, 'gamma': 1.2445984625620392, 'subsample': 0.869032247063718, 'colsample_bytree': 0.7288937116240548, 'lambda': 0.18622422155447793, 'alpha': 2.1265094899567948e-08}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:20:10] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:20:10] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:21:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:21:41,243] Trial 26 finished with value: 0.663072202522182 and parameters: {'learning_rate': 0.0036599775540920518, 'max_depth': 13, 'min_child_weight': 2, 'gamma': 4.508431912772047, 'subsample': 0.7044127634559103, 'colsample_bytree': 0.6175243142248699, 'lambda': 0.0007841636280212581, 'alpha': 1.849355134269005}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:22:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:22:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:22:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:22:43,704] Trial 27 finished with value: 0.6896542772738836 and parameters: {'learning_rate': 0.12702849841476302, 'max_depth': 11, 'min_child_weight': 5, 'gamma': 3.3407825033290166, 'subsample': 0.7689526448140935, 'colsample_bytree': 0.7932907305381132, 'lambda': 3.4803547246891195, 'alpha': 9.457336709627185}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:23:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:23:09] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:24:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:24:16,292] Trial 28 finished with value: 0.6940673170275841 and parameters: {'learning_rate': 0.19155765487821794, 'max_depth': 14, 'min_child_weight': 4, 'gamma': 2.745647076789745, 'subsample': 0.8446576420431842, 'colsample_bytree': 0.7070637781374347, 'lambda': 0.0042348795452834456, 'alpha': 0.08753459058493436}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:24:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:24:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:25:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:25:54,080] Trial 29 finished with value: 0.6941919205029827 and parameters: {'learning_rate': 0.19791660492261268, 'max_depth': 14, 'min_child_weight': 4, 'gamma': 1.9806913461729772, 'subsample': 0.9544867728905355, 'colsample_bytree': 0.5563004619390595, 'lambda': 0.00382418948400136, 'alpha': 0.034808307751976846}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:26:20] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:26:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:29:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:29:54,457] Trial 30 finished with value: 0.6828374288072852 and parameters: {'learning_rate': 0.02377417398897158, 'max_depth': 15, 'min_child_weight': 6, 'gamma': 1.9923465078727962, 'subsample': 0.9501053711952074, 'colsample_bytree': 0.5405035454809756, 'lambda': 0.00022938174069883022, 'alpha': 0.002694554610657704}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:30:20] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:30:20] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:31:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:31:18,776] Trial 31 finished with value: 0.6928835840112973 and parameters: {'learning_rate': 0.20030709777833575, 'max_depth': 14, 'min_child_weight': 4, 'gamma': 2.696221702006592, 'subsample': 0.8471322600311265, 'colsample_bytree': 0.5253540277036004, 'lambda': 0.0052457185423313, 'alpha': 0.05615022228990301}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:31:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:31:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:33:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:33:52,416] Trial 32 finished with value: 0.6964970847978568 and parameters: {'learning_rate': 0.10504622232506088, 'max_depth': 14, 'min_child_weight': 5, 'gamma': 1.4131415378402432, 'subsample': 0.970489762280503, 'colsample_bytree': 0.5943569207752692, 'lambda': 0.005867993487221891, 'alpha': 0.17410109817978411}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:34:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:34:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:35:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:35:58,791] Trial 33 finished with value: 0.68774888246258 and parameters: {'learning_rate': 0.05216082797245574, 'max_depth': 13, 'min_child_weight': 5, 'gamma': 1.3288227613848682, 'subsample': 0.9669833461400913, 'colsample_bytree': 0.5029375896608826, 'lambda': 9.511082741870618, 'alpha': 0.438988342996812}. Best is trial 2 with value: 0.6968605116011027.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:36:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:36:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:38:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:38:48,447] Trial 34 finished with value: 0.6977794622321675 and parameters: {'learning_rate': 0.10805121516174788, 'max_depth': 15, 'min_child_weight': 5, 'gamma': 0.9564171503718895, 'subsample': 0.9965690185435883, 'colsample_bytree': 0.5845498491619886, 'lambda': 0.17251274011969964, 'alpha': 2.7643529580514006}. Best is trial 34 with value: 0.6977794622321675.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:39:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:39:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:41:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:41:51,741] Trial 35 finished with value: 0.6982726843222868 and parameters: {'learning_rate': 0.1151249344124766, 'max_depth': 15, 'min_child_weight': 6, 'gamma': 0.7504896499336802, 'subsample': 0.9949184069703332, 'colsample_bytree': 0.6472526973613586, 'lambda': 0.28795200191743736, 'alpha': 2.3050517416198075}. Best is trial 35 with value: 0.6982726843222868.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:42:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:42:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:45:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:45:10,358] Trial 36 finished with value: 0.69709933492895 and parameters: {'learning_rate': 0.09392837909030696, 'max_depth': 15, 'min_child_weight': 6, 'gamma': 0.7299001411868032, 'subsample': 0.9960513193067692, 'colsample_bytree': 0.6460880831861778, 'lambda': 0.24073081202282406, 'alpha': 2.552533007872343}. Best is trial 35 with value: 0.6982726843222868.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:45:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:45:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:49:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:49:18,237] Trial 37 finished with value: 0.6870427961019879 and parameters: {'learning_rate': 0.03702216363200903, 'max_depth': 15, 'min_child_weight': 6, 'gamma': 0.8774163653212869, 'subsample': 0.9935478581538566, 'colsample_bytree': 0.6454239953247976, 'lambda': 0.7089979136928257, 'alpha': 3.103827438986005}. Best is trial 35 with value: 0.6982726843222868.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:49:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:49:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:53:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:53:12,422] Trial 38 finished with value: 0.6957390803225153 and parameters: {'learning_rate': 0.0730723633865109, 'max_depth': 15, 'min_child_weight': 8, 'gamma': 0.3821091839468098, 'subsample': 0.9261539420317252, 'colsample_bytree': 0.6456496359810975, 'lambda': 0.16222603718723855, 'alpha': 1.2062249385591045e-05}. Best is trial 35 with value: 0.6982726843222868.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:53:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:53:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:56:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-05-22 17:56:44,278] Trial 39 finished with value: 0.6960921235028114 and parameters: {'learning_rate': 0.08544255506756174, 'max_depth': 15, 'min_child_weight': 7, 'gamma': 0.9658991300904485, 'subsample': 0.995820134323412, 'colsample_bytree': 0.5880333250168605, 'lambda': 0.6902424653815915, 'alpha': 2.1746005409913125e-06}. Best is trial 35 with value: 0.6982726843222868.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:57:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:57:09] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p3-t_vTzDtSi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}